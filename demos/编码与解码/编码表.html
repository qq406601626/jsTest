<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>
【一般提到的编码，指的就是不同的编码表。】


ASCII编码表：
使用8位表示一个字符。
最基础的编码表，有128个字符。
128个字符用7个位刚好可以表示，计算机存储的最小单位是bit，即8位，ASCI码中最高位设置为0，用剩下的7位表示字符。
这7位可以看做数字0到127，ASCII码规定了从0到127个。
32到126表示可打印字符，即可以显示出来的字符。
因为ASCII编码表示的字符数量有限，所以为了表示其它国家的字符，就衍生出了其它的编码表。


ISO-8859-1编码：
使用8位表示一个字符。
IS0 8859-1又称Latin-1，主要用于西欧。
因为西欧的文字也都是字母拼接，只不过不是26个英文字母罢了，其中0到127与Asci一样。
128到255（即1xxx xxxx）规定了不同的含义：
128到159表示一些不可打印的控制字符。
160到255表示一些西欧字符。


Windows-1252编码：
使用8位表示一个字符。
IS0 8859-1虽然号称是标准，用于西欧国家，但它连欧元这个符号都没有，因为欧元比较晚，而标准比较早。实际使用中更为广泛的是Windows-1252编码。
这个编码与IS08859-1基本是一样的。
区别只在于数字128到159：
Windows-1252在128到159中用一些可打印字符替换掉了原来一些不可打印的字符。


GB2312编码：
使用两个字节表示汉字。
美国和西欧一个字符用就8位表示就够了（因为00000000到11111111表示的256种情况可以容纳并代表所有欧美的字符）。
但中文显然是不够的。中文第一个标准是GB2312。
GB2312标准主要针对的是【简体中文】常见字符，包括约7000个汉字，不包括一些罕见词，不包括繁体字。
GB2312固定使用两个字节表示汉字。
在这两个字节中：
如果最高位都是1，则认为是GB2312编码。
如果是0，就认为是Ascii字符。
即：GB2312编码在解析时，如果最高是0，则直接走Ascii编码表进行解析，以节省效率。如果最高位是1，则两两字节进行解析。
在这两个字节中：
其中第一个字节范围是1010 0001(十进制161)-1111 0111(十进制247)，第二个字节范围是1010 0001(十进制161)-1111 1110(十进制254)。
比如，"贤哥"的GB2312编码是：贤：CF CD，哥：B8 E7。


GBK编码表：
使用两个字节表示汉字。
GBK建立在GB2312的基础上，向下兼容GB2312，也就是说，GB2312编码的字符的二进制表示，在GBK编码里是完全一样的。
GBK增加了一万四千多个汉字，共计约21000汉字，其中包括繁体字。
GBK同样使用固定的两个字节表示，其中第一个字节范围是1000 0001(十进制129)-1111 1110(十进制254),第二个字节范围是01000000(十进制64)-01111110(十进制126)和1000 0000(十进制128)-1111 1110(十进制254)。
需要注意的是，第二个字节是从64开始的(64属于byte正数范围，和ASCI的编码重合了)，也就是说，第二个字节最高位可能为0。那怎么知道它是汉字的一部分，还是一个ASCII字符呢?
其实很简单，因为汉字是用固定两个字节表示的，在解析二进制流的时候，如果第一个字节的最高位为1，那么就将下一个字节读进来一起解析为一个汉字，而不用考虑它的最高位，解析完后，跳到第三个字节继续解析。

GB18030编码：
使用两个字节或者四个字节。
GB18030向下兼容GBK，增加了五万五千多个字符，共七万六千多个字符，如果继续使用固定两个字节表示，那么最多可以表示65535个汉字，不能覆盖七万多个汉字，所以使用两个字节或者四个字节。
包括了很多少数民族字符，以及中日韩统一字符。
用两个字节已经表示不了GB18030中的所有字符。
GB18030使用变长编码，有的字符是两个字节，有的是四个字节。
在两字节编码中，字节表示范围与GBK一样。
在四字节编码中，第一个字节的值从10000001(十进制129)到11111110(十进制254)，第二个字节的值从0011 0000(十进制48)到0011 1001(十进制57)，第三个字节的值从1000 0001(十进制129)到11111110(十进制254)，第四个字节的值从0011 0000(十进制48)到0011 1001(十进制57)。
解析二进制时，如何知道是两个字节还是四个字节表示一个字符呢?
很简单，看第二个字节的范围，如果是48到57就是四个字节表示，因为两个字节编码中第二字节都比这个大。所以这样综合说明GB18030兼容GBK，兼容GB2312，兼容ASCII，但是GB18030，GBK，GB2312这三个编码和ISO8859-1是不兼容的哦。
过程解析：
1、编码：在输入一个文字后，会把该文字根据编码表用二进制表示该字符的码点，在GB18030编码中，根据码点值范围，可能用16位（两个字节）表示也有可能用32位（四个字节）表示。
2、解码：如果是用GB18030编码规则，会根据第二个字节的区间范围，如果在0011 0000(十进制48)到0011 1001(十进制57)，就把四个字节作为整体解析，否则把两个字节作为整体解析。


编码表汇总：
其他编码都是兼容Ascii，如果最高位是0，就用ASCII编码，如果最高位是1，就用指定的编码。
每种系列的编码都是向前兼容的。但是不同系列的编码互不兼容。


Unicode编码表：
以上我们介绍了中文和西欧的字符与编码，但世界上还有很多的国家的字符，每个国家的各种计算机厂商都对自己常用的字符进行编码，在编码的时候基本忽略了别的国家的字符和编码，甚至忽略了同一国家的其他计算机厂商，这样造成的结果就是，出现了太多的编码，且互相不兼容。
世界上所有的字符能不能统一编码呢?可以，这就是Unicode。
Unicode 做了一件事，就是给世界上所有字符都分配了一个唯一的数字编号。
这个编号范围从0x000000（十六进制）到0x10FFFF（十六进制），包括110多万。
但大部分常用字符都 在0x0000到0xFFFF之间，即65536个数字之内。
【每个字符都有一个Unicode编号，这个编号一般写成16进制，在前面加U+】。
大部分中文的编号范围在U+4E00到U+9FA5，例如，"贤"的Unicode是U+8D24.
Unicode就做了这么一件事，就是给所有字符分配了唯一数字编号。
它并没有规定这个编号怎么对应到二进制表示，这是与上面介绍的其他编码不同的，其他编码都既规定了能表示哪字符，又规定了每个字符对应的二进制是什么，而Unicode本身只规定了每个字符的数字编号是多少。
（即Unicode给所有字符都制定了唯一编号，但是没有规定这些编号如何存储在计算机中。其它编码规则都是通过二进制存储在计算机中）。
那编号怎么对应到二进制表示呢?有多种方案，主要有UTF-32,UTF-16和UTF-8（UTF：Unicode Transformation Format的缩写）。
为什么计算机不直接把Unicode的编号以二进制存储在计算机中呢，因为Unicode最大的编号差不多是110万，如果以二进制存储在计算机中，那么非常占用空间。


UTF-32：
固定四个字节。
把Unicode编码转成二进制形式存在计算机中。


UTF-16（windows默认的Unicode编码就是UTF-16，在没有特殊说明的情况下，常说的Unicode编码可以理解为UTF-16，而且是UTF-16 BE编码，常说的中文转Unicode指的是把中文的Unicode码点根据UTF-16进行编码）：
可变长度，码点小于U+FFFF（65535）的为2个字节，码点大于U+FFFF（65535）的为四个字节。
因为UTF-32固定采用四个字节，即便编码为0的也会使用32位（四个字节）存储，太浪费空间，所以就有了UFT-16。
在了解 UTF-16 编码方式之前，先了解一下另外一个概念--"平面"：
在上面的介绍中，提到了Unicode 是一本很厚的字典，它将全世界所有的字符定义在一个集合里。这么多的字符不是一次性定义的，而是分区定义。每个区可以存放65536个(2^16 )字符，称为一个平面(plane)。
目前,一共有 17个(2^5 )平面(65536*17=1,114,112也就是110多万)，也就是说，整个 Unicode 字符集的大小现在是 2^21（换言之，最多使用21个bit就可以代表所有的Unicode字符）。
【最前面的 65536 个字符位，称为基本平面(简称 BMP)，它的码点范围是从0到 2^16-1，写成 16 进制就是从U+0000 到 U+FFFF。 所有最常见的字符都放在这个平面，这是 Unicode 最先定义和公布的一个平面】。
【剩下的字符都放在辅助平面(简称 SMP )，码点范围从 U+010000 到 U+10FFFF】。
基本了解了平面的概念后，再说回到 UTF-16。
UTF-16 编码介于 UTF-32 与 UTF-8之间，同时结合了定长和变长两种编码方法的特点。它的编码规则很简单:
【基本平面的字符占用2个字节，辅助平面的字符占用4个字节】。
【也就是说，UTF-16 的编码长度要么是2个字节(U+0000 到 U+FFFF，也就是)，要么是4个字节(U+010000 到U+10FFFF)】。
那么问题来了，当我们遇到两个字节时，到底是把这两个字节当作一个字符还是与后面的两个字节一起当作一个字符呢?
为了将两个字节的UTF-16编码与四个字节的UTF-16编码区分开来，Unicode编码的设计者将0xD800-0xDFFE保留下来，并称为代理区(Surrogate):
辅助平面的字符位共有 2^20个，因此表示这些字符至少需要 20 个二进制位。UTF-16 将这 20 个二进制位分成两半，
前 10 位映射在 U+D800 到 U+DBFE，称为高代理位(H)，
后10 位映射在 U+DC00 到 U+DEFE，称为低代理位(L)。这意味着，一个辅助平面的字符，被拆成两个基本平面的字符表示。
计算规则：
1、如果字符的码点小于0xFFFF（即codePointAt小于65535），那么就用两个字节表示。
2、如果字符的码点大于0xFFFF，那么：
    2.1：用该码点减去0x10000，然后把得到的值转化为二进制。并且该二进制固定用20位（bit）表示，不足20位的前面补0。
    2.2：把该二进制分成两段，前十位为一段，后十位为一段。
    2.3：在前十位的前面补上110110，目的是让前十位落在U+D800到U+DBFF高代理位区间。
    2.4：在后十位的前面补上110111，目的是让后十位落在U+DC00到U+DFFF低代理位区间。
    2.5：分别把拼接后的前十位的二进制和后十位的二进制转成16进制，则得到该字符的编码，然后存储在计算机中。

和UFT-32一样，UTF-16也有大端和小端：



UTF-8：
变长字节表示，字节个数从1到4个不等。
UTE-16比UTF-32节省了很多空间，但是任何一个字符都至少需要两个字节表示，对于美国和西欧国家而言，还是很浪费的，所以有了UTF-8。
UTF-8就是使用变长字节表示，每个字符使用的字节个数与其unicode编号的大小有关，编号小的使用的字节就少，编号大的使用的字节就多，使用的字节个数从1到4个不等，具体如下：
-----------------------------------------------------------
Unicode编码（十六进制）             UTF-8字节流（二进制）
000000-00007F（127）              0xxxxxxx（一个字节）
000080-0007FF（2047）             110xxxxx  10xxxxxx（两个字节）
000800-00FFFF（65535）            1110xxxx  10xxxxxx  10xxxxxx（三个字节）
010000-10FFFF（1114111）          11110xxx  10xxxxxx  10xxxxxx  10xxxxxx（四个字节）
-----------------------------------------------------------


nodejs默认使用utf-8：从1个字符到四个字符，即8位到32位。
chromejs默认使用utf-16：要么两个字符，要么四个字符，即16位或者32位。
在chrome控制台中使用\u表示字符时：
1、比如“你”的16进制位4f60，那么在表示时使用\u4f60。
2、比如“a”的16进制位61，那么在表示时使用\u0061（因为61位一个字符，而utf-16使用2个字节或四个字节表示，所以不足两个字节的需要补0）。



EditPlus默认编码为GBK，该编码表兼容GB2312。


</body>
</html>